# 딥러닝을 해킹하는 적대적 공격

딥러닝 모델을 의도적으로 헷갈리게 하는 적대적 예제에 대해 알아보고 적대적 예제를 생성하는 방법인 적대적 공격(adversarial attack)을 알아봅니다.

* 적대적 공격 이란
* 적대적 공격의 종류
* [FGSM 공격](fgsm_attack.ipynb)
    * 학습된 모델 불러오기
    * 공격할 미이지 불러오기
    * 공격 전 성능 확인하기
    * FGSM 공격 함수 정의
    * 적대적 예제 생성
    * 적대적 예제 성능 확인